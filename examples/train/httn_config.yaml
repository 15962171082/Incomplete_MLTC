num_head_labels: 84
sample_pre_num: 8
sample_times: 10
task_name: httn_head${num_head_labels}_prenum${sample_pre_num}_times${sample_times}
device: 0

num_all_labels: 96    # CCKS:96 AAPD:54

max_seq_len: 256
data_type: CCKS     # AAPD  CCKS

data_dir: data/${data_type}

# plm_dir: /data2/jfren/PLM_Model/bert-base-cased
plm_dir: /data2/jfren/PLM_Model/bert-base-chinese

train_filepath: ${data_dir}/raw_data/train.jsonl
# train_filepath: ${data_dir}/lose_real/train_0.1.jsonl
# train_filepath: ${data_dir}/lose_fake/train_0.1.jsonl
unlabel_filepath: ${data_dir}/raw_data/unlabel.jsonl
dev_filepath: ${data_dir}/raw_data/dev.jsonl
test_filepath: ${data_dir}/raw_data/test.jsonl

output_dir: outputs/${data_type}
task_dir: ${output_dir}/${task_name}
label2id_filepath: ${data_dir}/label2id.json

# training
load_train_data: true
load_test_data: true
load_dev_data: true
final_eval_model_filepath: ${task_dir}/ckpt/HTTN.best.pth
best_eval_model_filepath: ${task_dir}/ckpt/HTTN.best.pth

debug_mode: false
warmup_proportion: 0.1
gradient_accumulation_steps: 1

random_seed: 1227
transfor_train_num_epochs: 1000
num_epochs: 10
num_early_stop: 20
train_batch_size: 16
eval_batch_size: 16
learning_rate: !!float 2e-5
save_best_ckpt: true

# logging
only_master_logging: true

# model
dropout: 0.5
num_filters: 200
kernel_sizes: [1, 3, 5, 7]
threshold: 0.5
